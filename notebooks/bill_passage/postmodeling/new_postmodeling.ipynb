{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postmodeling with the new template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.general import get_db_conn, get_boto3_session, load_model_s3, load_matrix_s3\n",
    "from src.utils import project_constants as constants\n",
    "\n",
    "from triage.component.catwalk.storage import ProjectStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_file = '../conf/local/credentials.yaml'\n",
    "db_conn=get_db_conn(cred_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Score distributions\n",
    "\n",
    "We will plot score distribution for each model id (within each model group) and train_end_time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Performance\n",
    "\n",
    "We will look at the performance of each model group at each train_end_time. The performance metric is defined earlier and we cn also specify baselines in the baseline query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2A. Performance over time\n",
    "\n",
    "We will look at the performance of each model group at each train_end_time. The performance metric is defined earlier and we cn also specify baselines in the baseline query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2B. Precision Recall Curves\n",
    "\n",
    "for each model id (within each model group) and train_end_time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2C. ROC Curves\n",
    "for each model id (within each model group) and train_end_time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3A. Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3B. We can also look at the feature space to see which features are correlated and which features are mostly missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3C Feature Group Importances\n",
    "\n",
    "We can see which feature groups are more important in a few different ways:\n",
    "1. max feature importance over each feature group\n",
    "2. LOO and LOI performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Cross-tabs\n",
    "\n",
    "Cross-tabs for each model id (within each model group) and train_end_time here Crosstabs. crosstabs will generate a table with the differences in feature distributions across the top k scored entites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Error Analysis\n",
    "\n",
    "This function error_analysis runs a Decision Tree for each model based in one (or several) thresholds passed to the Postmodeling configuration file. The function will return the tree plot (saved as pdf under the /error_analysis).\n",
    "\n",
    "The function will make four comparisons:\n",
    "\n",
    "False Postives (1) vs. Rest of entities classified (0)\n",
    "False Negatives (1) vs. Rest of entities classified (0)\n",
    "False Positives (1) vs. True Negatives (0)\n",
    "False Negatives (1) vs. True Positives (0)\n",
    "This set of analysis will serve to understand why some classification errors arise, and give some hints on how our model is performing.\n",
    "\n",
    "for each model id (within each model group) and train_end_time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Bias Analysis\n",
    "\n",
    "for each model id (within each model group) and train_end_time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compare predicted lists\n",
    "\n",
    "We can compare model_group_ids by comparing which entities they score in the top k. We can do that by looking at overlap (Jaccard Similarity Index) or by the rankings (Rank Correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1A. Jaccard Similarity\n",
    "\n",
    "show one matrix for each train_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1B. Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compare Feature Importances\n",
    "\n",
    "We can compare model_group_ids by comparing which features they found as important. We can do that by looking at overlap (Jaccard Similarity Index) or by the rankings (Rank Correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2A. Jaccard Similarity\n",
    "\n",
    "show one matrix for each train_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2B. Rank Correlation\n",
    "\n",
    "show one matrix for each train_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compare Cross-Tabs\n",
    "\n",
    "We will plot score distribution for each model id (within each model group) and train_end_time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Bias\n",
    "\n",
    "for each model id (within each model group) and train_end_time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aclu",
   "language": "python",
   "name": "aclu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
